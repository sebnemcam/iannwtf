{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. get mnist from tensorflow_datasets\n",
    "mnist = tfds.load(\"mnist\", split =[\"train\",\"test\"], as_supervised=True)\n",
    "train_ds = mnist[0]\n",
    "val_ds = mnist[1]\n",
    "\n",
    "# 2. write function to create the dataset that we want\n",
    "def preprocess(data, batch_size, type):\n",
    "    # image should be float\n",
    "    data = data.map(lambda x, t: (tf.cast(x, float), t))\n",
    "    # image should be flattened\n",
    "    data = data.map(lambda x, t: (tf.reshape(x, (-1,)), t))\n",
    "    # image vector will here have values between -1 and 1\n",
    "    data = data.map(lambda x,t: ((x/128.)-1., t))\n",
    "    # we want to have two mnist images in each example\n",
    "    # this leads to a single example being ((x1,y1),(x2,y2))\n",
    "    zipped_ds = tf.data.Dataset.zip((data.shuffle(2000), data.shuffle(2000)))\n",
    "\n",
    "    if type == 'greater_equal':\n",
    "        # map ((x1,y1),(x2,y2)) to (x1,x2, y1==y2*) *boolean\n",
    "        zipped_ds = zipped_ds.map(lambda x1, x2: (x1[0], x2[0], x1[1] + x2[1] >= 5))\n",
    "        # transform boolean target to int\n",
    "        zipped_ds = zipped_ds.map(lambda x1, x2, t: (x1,x2, tf.cast(t, tf.int32)))\n",
    "        # batch the dataset\n",
    "        zipped_ds = zipped_ds.batch(batch_size)\n",
    "        # prefetch\n",
    "        zipped_ds = zipped_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    elif type == 'subtract':\n",
    "        # map ((x1,y1),(x2,y2)) to (x1,x2, y1 - y2)\n",
    "        zipped_ds = zipped_ds.map(lambda x1, x2: (x1[0], x2[0], abs(x1[1] - x2[1])))\n",
    "        # batch the dataset\n",
    "        zipped_ds = zipped_ds.batch(batch_size)\n",
    "        # prefetch\n",
    "        zipped_ds = zipped_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return zipped_ds\n",
    "\n",
    "greater_equal_train_ds = preprocess(train_ds, batch_size=32, type='greater_equal')\n",
    "greater_equal_val_ds = preprocess(val_ds, batch_size=32, type=\"greater_equal\")\n",
    "\n",
    "subtract_train_ds = preprocess(train_ds, batch_size=32, type='subtract') #train_ds.apply(preprocess)\n",
    "subtract_val_ds = preprocess(val_ds, batch_size=32, type=\"subtract\") \n",
    "\n",
    "for img1, img2, label in greater_equal_train_ds.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwinMNISTModel(tf.keras.Model):\n",
    "\n",
    "    # 1. constructor\n",
    "    def __init__(self, type):\n",
    "        super().__init__()\n",
    "        # inherit functionality from parent class\n",
    "\n",
    "        # optimizer, loss function and metrics\n",
    "        self.metrics_list = [tf.keras.metrics.BinaryAccuracy(),\n",
    "                             tf.keras.metrics.Mean(name=\"loss\")]\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        #type-dependent settings\n",
    "        if type == \"greater_equal\":\n",
    "            self.loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "            self.out_layer = tf.keras.layer.Dense(1,activation=tf.nn.sigmoid)\n",
    "        elif type == \"subtract\":\n",
    "            self.loss_function = tf.keras.losses.MeanSquaredError()\n",
    "            self.out_layer = tf.keras.layer.Dense(1,activation=tf.nn.relu)\n",
    "\n",
    "        #same layers for both types\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # 2. call method (forward computation)\n",
    "    def call(self, images, training=False):\n",
    "        img1, img2 = images\n",
    "        \n",
    "        img1_x = self.dense1(img1)\n",
    "        img1_x = self.dense2(img1_x)\n",
    "        \n",
    "        img2_x = self.dense1(img2)\n",
    "        img2_x = self.dense2(img2_x)\n",
    "        \n",
    "        combined_x = tf.concat([img1_x, img2_x], axis=1)\n",
    "        \n",
    "        return self.out_layer(combined_x)\n",
    "\n",
    "\n",
    "\n",
    "    # 3. metrics property\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "        # return a list with all metrics in the model\n",
    "\n",
    "\n",
    "\n",
    "    # 4. reset all metrics objects\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "\n",
    "\n",
    "\n",
    "    # 5. train step method\n",
    "    def train_step(self, data):\n",
    "        img1, img2, label = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self((img1, img2), training=True)\n",
    "            loss = self.loss_function(label, output)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        # update the state of the metrics according to loss\n",
    "        # return a dictionary with metric names as keys and metric results as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(...):\n",
    "\n",
    "    # 1. iterate over epochs\n",
    "\n",
    "\n",
    "        # 2. train steps on all batches in the training data\n",
    "\n",
    "\n",
    "\n",
    "        # 3. log and print training metrics\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            ...\n",
    "\n",
    "\n",
    "\n",
    "        # 4. reset metric objects\n",
    "\n",
    "\n",
    "\n",
    "        # 5. evaluate on validation data\n",
    "\n",
    "\n",
    "\n",
    "        # 6. log validation metrics\n",
    "\n",
    "        with val_summary_writer.as_default():\n",
    "            ...\n",
    "\n",
    "        # 7. reset metric objects\n",
    "\n",
    "\n",
    "    # 8. save model weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ed48ea506cc6e3e556dd6397f80ebb7e16428b1b3cf2350b34407ef5f1d989a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
